# Streaming LLM Architecture

## Overview

The streaming LLM architecture enables real-time response generation with progressive audio synthesis, dramatically improving user experience by providing immediate feedback while maintaining natural audio quality.

## Architecture Components

### 1. StreamingCommandParser

**File**: `internal/llm/streaming_command_parser.go`

The core component that handles streaming responses from Ollama LLM.

#### Key Features

- **Token-Level Streaming**: Receives tokens as they're generated by the LLM
- **Dual Channel Output**:
  - `VisualTokens`: Immediate visual feedback
  - `AudioPhrases`: Buffered phrases for natural TTS
- **Intelligent Buffering**: Detects natural speech boundaries
- **Fallback Support**: Graceful degradation to traditional parsing

#### Channel Architecture

```go
type StreamingResult struct {
    TokenStream    chan string           // Raw token stream
    FinalCommand   chan *Command         // Final parsed command
    ErrorChan      chan error            // Error notifications
    Cancel         context.CancelFunc    // Cancellation function
    VisualTokens   chan string           // Immediate UI feedback
    AudioPhrases   chan string           // Natural phrase boundaries
    Metrics        *StreamingMetrics     // Performance tracking
}
```

#### Phrase Buffering

The `PhraseBuffer` component intelligently buffers tokens until natural speech boundaries:

- **Sentence endings**: `.`, `!`, `?`
- **Natural pauses**: `, and`, `, then`, `, so`
- **Connectors**: `, but`, `, however`, `, because`
- **Time limits**: Maximum 2-second buffering to prevent delays
- **Token limits**: Maximum 50 tokens to prevent runaway buffering

### 2. StreamingAudioPipeline

**File**: `internal/llm/streaming_audio_pipeline.go`

Manages progressive TTS synthesis with parallel processing and sequence ordering.

#### Architecture

```
Phrase Stream ‚Üí Synthesis Queue ‚Üí Worker Pool ‚Üí Audio Sequencer ‚Üí Audio Output
                                      ‚Üì
                               Parallel TTS Workers
                                  (configurable)
```

#### Key Components

- **Phrase Processor**: Queues phrases for synthesis
- **Worker Pool**: Parallel TTS synthesis (default: 3 workers)
- **Audio Sequencer**: Ensures correct ordering of audio chunks
- **Pipeline Context**: Per-session state management

#### Performance Features

- **Parallel Processing**: Multiple TTS requests simultaneously
- **Sequence Ordering**: Maintains phrase order despite parallel synthesis
- **Memory Management**: Proper cleanup of resources
- **Error Handling**: Graceful handling of TTS failures

### 3. StreamingInterruptHandler

**File**: `internal/llm/streaming_interrupt_handler.go`

Manages graceful interruption and cleanup of streaming sessions.

#### Interrupt Scenarios

- **User Interruption**: New wake word detected
- **Command Collision**: Multiple simultaneous commands
- **Timeout**: Stream taking too long
- **Error Recovery**: LLM or TTS failures
- **System Shutdown**: Clean service termination

#### Cleanup Process

1. **Signal Cancellation**: Cancel all active contexts
2. **Grace Period**: 500ms for graceful shutdown
3. **Resource Cleanup**: Drain channels, stop workers
4. **Force Termination**: Last resort cleanup
5. **Session Removal**: Clean up tracking data

#### Session Management

```go
type StreamingSession struct {
    ID               string
    Context          context.Context
    Cancel           context.CancelFunc
    StreamingResult  *StreamingResult
    AudioPipeline    *PipelineContext
    CreatedAt        time.Time
    InterruptedAt    *time.Time
    CleanupCompleted bool
    InterruptReason  string
}
```

### 4. StreamingMetricsCollector

**File**: `internal/llm/streaming_metrics.go`

Comprehensive performance monitoring and health assessment.

#### Metrics Tracked

- **Latency Metrics**:
  - First token time
  - First phrase time
  - Total completion time
- **Throughput Metrics**:
  - Tokens per second
  - Phrases per minute
  - Audio chunks generated
- **Quality Metrics**:
  - Buffer overflow count
  - Interrupt frequency
  - Error rates
- **Health Assessment**:
  - System health status
  - Performance trends
  - Optimization recommendations

#### Health Status Levels

- **Healthy**: Error rate < 10%, latency < 2s
- **Warning**: Error rate 10-20% or latency 2-5s
- **Critical**: Error rate > 20% or latency > 5s
- **Unknown**: Insufficient data

## Configuration

### Streaming Settings

```go
type StreamingConfig struct {
    Enabled              bool          `yaml:"enabled"`
    OllamaURL           string        `yaml:"ollama_url"`
    Model               string        `yaml:"model"`
    MaxBufferTime       time.Duration `yaml:"max_buffer_time"`
    MaxTokensPerPhrase  int           `yaml:"max_tokens_per_phrase"`
    AudioConcurrency    int           `yaml:"audio_concurrency"`
    VisualFeedbackDelay time.Duration `yaml:"visual_feedback_delay"`
    InterruptTimeout    time.Duration `yaml:"interrupt_timeout"`
    FallbackEnabled     bool          `yaml:"fallback_enabled"`
    MetricsEnabled      bool          `yaml:"metrics_enabled"`
}
```

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `STREAMING_ENABLED` | `true` | Enable streaming responses |
| `STREAMING_OLLAMA_URL` | `http://localhost:11434` | Ollama endpoint |
| `STREAMING_MODEL` | `llama3.2:3b` | LLM model |
| `STREAMING_MAX_BUFFER_TIME` | `2s` | Phrase buffer timeout |
| `STREAMING_MAX_TOKENS_PER_PHRASE` | `50` | Buffer size limit |
| `STREAMING_AUDIO_CONCURRENCY` | `3` | TTS worker count |
| `STREAMING_VISUAL_FEEDBACK_DELAY` | `50ms` | UI responsiveness |
| `STREAMING_INTERRUPT_TIMEOUT` | `500ms` | Cleanup timeout |
| `STREAMING_FALLBACK_ENABLED` | `true` | Graceful degradation |
| `STREAMING_METRICS_ENABLED` | `true` | Performance tracking |

## Performance Characteristics

### Latency Improvements

- **First Token**: ~200ms (vs ~2s traditional)
- **First Phrase**: ~500ms (vs ~2s traditional)
- **User Perception**: Immediate responsiveness

### Memory Usage

- **Base Overhead**: ~50MB for streaming components
- **Per Session**: ~5MB average, ~20MB peak
- **Cleanup**: Automatic resource management

### Scalability

- **Concurrent Sessions**: Tested up to 10 simultaneous streams
- **Throughput**: 150+ tokens/second average
- **Audio Processing**: 3-5 parallel TTS syntheses

## Error Handling

### Fallback Strategy

1. **Stream Initialization Failure**: Fall back to traditional parsing
2. **Mid-Stream Errors**: Complete current phrase, switch to fallback
3. **TTS Failures**: Continue with text-only responses
4. **Network Issues**: Retry with exponential backoff
5. **Resource Exhaustion**: Graceful degradation

### Error Recovery

```go
func (scp *StreamingCommandParser) handleStreamError(err error) {
    log.Printf("Streaming error: %v", err)

    if scp.fallbackEnabled {
        // Switch to traditional parsing
        cmd, fallbackErr := scp.fallbackParser.ParseCommand(transcription)
        if fallbackErr == nil {
            return scp.createFallbackResult(cmd)
        }
    }

    // Return error to caller
    return nil, fmt.Errorf("streaming failed: %w", err)
}
```

## Testing Strategy

### Unit Tests

- **Component Isolation**: Mock external dependencies
- **Error Scenarios**: Network failures, timeouts, malformed responses
- **Resource Cleanup**: Verify no goroutine leaks
- **Performance**: Latency and throughput benchmarks

### Integration Tests

- **End-to-End Streaming**: Full pipeline from LLM to audio
- **Interruption Handling**: Various cancellation scenarios
- **Concurrent Sessions**: Multiple simultaneous streams
- **Fallback Behavior**: Graceful degradation testing

### Performance Tests

- **Load Testing**: High concurrent session counts
- **Memory Profiling**: Resource usage under load
- **Latency Benchmarks**: Response time measurements
- **Stress Testing**: Resource exhaustion scenarios

## Deployment Considerations

### Feature Flags

- **Gradual Rollout**: Enable for subset of users
- **A/B Testing**: Compare streaming vs traditional performance
- **Kill Switch**: Rapid disable if issues arise
- **Per-Model Control**: Different settings per LLM model

### Monitoring

- **Health Endpoints**: `/api/streaming/health`
- **Metrics Export**: Prometheus/Grafana integration
- **Alert Thresholds**: Latency, error rate, resource usage
- **Performance Dashboards**: Real-time system monitoring

### Scaling

- **Horizontal Scaling**: Multiple hub instances
- **Load Balancing**: Session affinity considerations
- **Resource Limits**: Memory and CPU constraints
- **Auto-scaling**: Based on streaming session count

## Future Enhancements

### Planned Features

- **Voice Activity Detection**: Automatic interruption on user speech
- **Response Caching**: Cache common responses for instant delivery
- **Adaptive Buffering**: Dynamic buffer sizing based on network conditions
- **Multi-Model Support**: Different models for different response types
- **WebSocket Streaming**: Real-time web client updates

### Performance Optimizations

- **GPU Acceleration**: Faster TTS synthesis
- **Model Quantization**: Reduced memory usage
- **Response Preloading**: Anticipate common responses
- **Compression**: Reduce audio bandwidth
- **Edge Deployment**: Closer to end users

## Troubleshooting

### Common Issues

1. **High Latency**
   - Check Ollama model loading time
   - Verify TTS service performance
   - Review buffer timeout settings

2. **Memory Leaks**
   - Ensure proper context cancellation
   - Check goroutine cleanup
   - Monitor session lifecycle

3. **Audio Quality Issues**
   - Adjust phrase buffering boundaries
   - Check TTS synthesis parameters
   - Verify audio sequencing

4. **Fallback Activation**
   - Check Ollama connectivity
   - Review error logs
   - Verify configuration settings

### Debug Commands

```bash
# Check streaming health
curl http://localhost:3000/api/streaming/health

# Get performance metrics
curl http://localhost:3000/api/streaming/metrics

# View active sessions
curl http://localhost:3000/api/streaming/sessions

# Test streaming connection
go test -v ./internal/llm -run TestStreamingCommandParser
```

### Log Analysis

Key log patterns to monitor:

- `üåä Streaming command completed` - Successful completions
- `‚ö†Ô∏è Streaming test failed` - Fallback activation
- `üõë Interrupting streaming session` - User interruptions
- `‚ùå TTS synthesis failed` - Audio pipeline issues
- `üßπ Cleaned up streaming session` - Resource cleanup

## Security Considerations

### Input Validation

- **Transcription Sanitization**: Prevent injection attacks
- **Rate Limiting**: Prevent abuse of streaming endpoints
- **Session Management**: Secure session identifiers
- **Resource Limits**: Prevent resource exhaustion

### Data Privacy

- **Local Processing**: No cloud dependencies
- **Memory Cleanup**: Secure data disposal
- **Session Isolation**: No cross-session data leakage
- **Audit Logging**: Comprehensive access logs

---

*This document covers the streaming LLM architecture as implemented in Milestone 6. For implementation details, see the source code and unit tests in `internal/llm/streaming_*.go`.*